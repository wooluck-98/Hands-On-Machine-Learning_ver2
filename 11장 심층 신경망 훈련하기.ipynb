{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 심층 신경망을 훈련할때 마주하는 문제\n",
    " - 심층 신경망의 아래쪽으로 갈수록 그레이디언트가 점점 더 작아지거나 커지는 그레이디언트 또는 그레디언트 폭주 문제\n",
    " - 대규모 신경망을 위한 훈련 데이터가 충분하지 않거나 레이블을 만드는 작업에 비용이 너무 많이 든다.\n",
    " - 훈련이 극단적으로 느려진다.\n",
    " - 수백만 개의 파라미터를 가진 모델은 훈랸 세트에 과대적합될 위험이 매우 크다.(특히 훈련 샘플이 충분하지 않거나 잡음이 많은 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.1 그레이디언트 소실과 폭주 문제 \n",
    "\n",
    "* 역전파 알고리즘은 출력층에서 입력층으로 오차 그레이디언트를 전파하면서 진행된다. 알고리즘이 신경망의 모든 파라미터에 대한 오차 함수의 그레이디언트를 계산하면 경사 하강법 단계에서 이 그레이디언트를 사용하여 각 파라미터를 수정한다.\n",
    "* 깊은 심층신경망에서는 역전파 알고리즘이 입력층으로 전달됨에 따라 그래디언트가 점점 작아져 결국 가중치 매개변수가 업데이트 되지 않는 경우가 발생하게 된다. 이러한 문제를 그래디언트 소실(vanishing gradient)라고 한다.\n",
    "<img src = 'images/11_images/vanishing.png'>\n",
    "\n",
    "* 그래디언트 소실과는 반대로 역전파에서 그래디언트가 점점 커져 입력층으로 갈수록 가중치 매개변수가 기하급수적으로 커지게 되는 경우가 있는데 이를 그래디언트 폭주(exploding gradient)라고 하며, 이 경우에는 발산(diverse)하게되어 학습이 제대로 이루어지지 않는다.\n",
    "\n",
    "* 문제들의 원인은 그 당시에 많이 사용되는 로지스틱 시그모이드 활성화 함수와 그 당시 가장 인기있었던 가중치 초기화 방법의 조합이었다.\n",
    " - 이 활성화 함수와 초기화 방식을 사용했을 때 각 층에서 출력의 분산이 입력의 분산보다 더 크다는 것이 밝혀졌다.\n",
    " - 신경망의 위쪽으로 갈수록 층을 지날 때마다 분산이 계속 커져 가장 높은 층에서는 활성화 함수가 0이나 1로 수렴한다.\n",
    " - 로지스틱 함수를 보면 입력이 커지면 0이나 1로 수렴해서 기울기가 0에 매우 가까워지는 것을 알 수 있다. 그래서 역전파가 될 때 사실상 신경망으로 전파할 그레이디언트가 거의 없고, 조금 있는 그레이디언트는 최상위층에서부터 역전파가 진행되면서 점차 약해져서 실제로 아래쪽 층에는 아무것도 도달하지 않게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.1 글로럿과 He 초기화\n",
    "\n",
    "* 예측과 역전파할때 양방향 신호가 적절하게 흘러야 한다. 글로럿과 벤지오는 적잘한 신호가 흐르기 위해서는 각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 한다고 주장했다. 그리고 역방향에서 층을 통과하기 전과 후의 그레이디언트 분산이 동일해야한다.\n",
    "* 층의 입력(Fan-in)과 출력 연결 개수(Fan-out)가 같지 않다면 이 두 가지를 보장할 수 없다.\n",
    "* 하지만 글로럿과 벤지오는 실전에서 매우 잘 작동한다고 입증된 대안을 제안\n",
    "* 각 층의 연결 가중치를 밑의 방식대로 무작위로 초기화\n",
    "\n",
    "* 글로럿 초기화(Glorot Initialization) 또는 세이비어 초기화(Xavier Initialization)\n",
    "  - $ fan_{ang} = fan_{in} + fan_{out}/ 2 $\n",
    "  - 평균이 0이고 분산이 $\\sigma^2 = \\frac{1}{fan_{avg}}$ 인 정규 분포 또는 $r = \\sqrt{\\frac{3}{fan_{{avg}}}}$ 일 때 -r과 +r 사이의 균등분포\n",
    "  \n",
    "* 르쿤 초기화(LuCun Initialization)\n",
    "  - 평균이 0이고 분산이 $\\sigma^2 = \\frac{1}{fan_{in}}$ 인 정규 분포 또는 $r = \\sqrt{\\frac{3}{fan_{{in}}}}$ 일 때 -r과 +r 사이의 균등분포\n",
    "  * He 초기화(He Initialization)\n",
    "  - ReLU 활성화 함수에 대한 초기호 전략\n",
    "\n",
    "|초기화 전략 | 활성화 함수 | $\\sigma^2$ |\n",
    "|-----------|-------------|--------------|\n",
    "| 글로럿 | 활성화 함수 없음, 하이퍼볼릭 탄젠트, 로지스틱, 소프트맥스 | $\\frac{1}{fan_{avg}}$|\n",
    "|He | ReLU 함수와 그 변종들 | $\\frac{2}{fan_{in}}$|\n",
    "|로쿤 | SELU들 | $\\frac{1}{fan_{in}}$|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d8abd1370>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#케라스는 기본적으로 균등분포의 글로럿 초기화 사용\n",
    "#kernel_initializer = \"he_uniform\" / kernel_initializer = \"he_normal\"로 바꾸어 He 초기호 사용\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer = \"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d8abc0ac0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fan_in 대신 fan_out 기반의 균등분포 He 초기화\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale = 2., mode = 'fan_avg', distribution = 'uniform')\n",
    "keras.layers.Dense(10, activation = \"sigmoid\", kernel_initializer = he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.2 수렴하지 않는 활성화 함수\n",
    "\n",
    "* 대부분 생물학적 뉴런의 방식과 비슷한 시그모이드 활성화 함수가 최선의 선택일 것이라고 생각했었지만 다른 활성화 함수가 심층 신경망에서 훨씬 더 잘 작동한다는 사실이 밝혀졌다.\n",
    "* ReLU 함수는 특정 양수값에 수렴하지 않고 계산이 빠르다는 장점이 있다. 하지만 훈련하는 동안 일부 뉴런이 0이외의 값을 출력하지 않는 죽은 ReLU(Dying ReLU) 문제가 있다. 뉴런의 가중치가 바뀌어 훈련 세트에 있는 모든 샘플에 대해 입력의 가중치 합이 음수가 되며 뉴런이 죽게되어 경사 하강법이 더 작동하지 않게된다.\n",
    "* 이 문제를 해결하기 위해 LeakyReLU같은 함수의 변종을 사용\n",
    "* $LeakyReLU_a$(z) = max(az, z)\n",
    "* 하이퍼파라미터 a가 함수의 새는(Leaky)정도를 결정한다.\n",
    " - 새는 정도란 z < 0 일때 이 함수의 기울기, 일반적으로 0.01로 설정\n",
    " - 이 작은 기울기가 LeakyReLU를 절대 죽지 않게 만든다.\n",
    " \n",
    " <img src = \"images/11_images/leaky.PNG\">\n",
    "\n",
    "* RReLU(Randomized Leaky ReLU) : 훈련하는 동안 주어진 범위에서 a를 무작위로 선택하고 테스트시에는 평균을 사용\n",
    "* PReLU(Parametric Leaky ReLU) : 하이퍼파라미터인 $\\alpha$를 가중치 매개변수와 마찬가지로 $\\alpha$의 값도 학습되도록 역전파에 의해 $\\alpha$의 값이 변경되는 함수\n",
    " - PReLU는 대규모 이미지 데이터셋에서는 ReLU보다 성능이 좋았지만, 소규모 데이터셋에는 오버피팅될 위험이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha = 0.01):\n",
    "    return np.maximum(alpha * z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 5s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#LeakyReLU를 사용해 패션MNIST에서 신경망 훈련\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 965us/step - loss: 1.2819 - accuracy: 0.6229 - val_loss: 0.8886 - val_accuracy: 0.7160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 887us/step - loss: 0.7955 - accuracy: 0.7361 - val_loss: 0.7130 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 894us/step - loss: 0.6816 - accuracy: 0.7721 - val_loss: 0.6427 - val_accuracy: 0.7900\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 1s 834us/step - loss: 0.6217 - accuracy: 0.7945 - val_loss: 0.5900 - val_accuracy: 0.8064\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 1s 859us/step - loss: 0.5832 - accuracy: 0.8074 - val_loss: 0.5582 - val_accuracy: 0.8200\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 1s 867us/step - loss: 0.5553 - accuracy: 0.8157 - val_loss: 0.5350 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 1s 863us/step - loss: 0.5338 - accuracy: 0.8224 - val_loss: 0.5156 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 1s 855us/step - loss: 0.5172 - accuracy: 0.8272 - val_loss: 0.5079 - val_accuracy: 0.8280\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 1s 863us/step - loss: 0.5040 - accuracy: 0.8290 - val_loss: 0.4895 - val_accuracy: 0.8386\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 917us/step - loss: 0.4924 - accuracy: 0.8321 - val_loss: 0.4817 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PReLU 테스트하기\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 1.3461 - accuracy: 0.6209 - val_loss: 0.9255 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 970us/step - loss: 0.8197 - accuracy: 0.7355 - val_loss: 0.7305 - val_accuracy: 0.7630\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 978us/step - loss: 0.6965 - accuracy: 0.7694 - val_loss: 0.6564 - val_accuracy: 0.7882\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 980us/step - loss: 0.6330 - accuracy: 0.7909 - val_loss: 0.6003 - val_accuracy: 0.8048\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 989us/step - loss: 0.5917 - accuracy: 0.8056 - val_loss: 0.5656 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 979us/step - loss: 0.5618 - accuracy: 0.8133 - val_loss: 0.5406 - val_accuracy: 0.8236\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 961us/step - loss: 0.5390 - accuracy: 0.8205 - val_loss: 0.5196 - val_accuracy: 0.8312\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 985us/step - loss: 0.5213 - accuracy: 0.8257 - val_loss: 0.5113 - val_accuracy: 0.8314\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 974us/step - loss: 0.5070 - accuracy: 0.8289 - val_loss: 0.4916 - val_accuracy: 0.8378\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 973us/step - loss: 0.4945 - accuracy: 0.8316 - val_loss: 0.4826 - val_accuracy: 0.8394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ELU(Exponential Linear Unit) :  2105년 B.Xu et al.의 논문에서 제안된 활성화 함수\n",
    "$$\n",
    "\\text{ELU}_{\\alpha} = \\begin{cases} \\alpha \\left( \\exp{(x)} - 1 \\right) &amp; \\text{if }x &lt;0 \\\\ x &amp; \\text{if } x \\ge 0 \\end{cases}\n",
    "$$\n",
    "<img src = \"images/11_images/elu.PNG\">\n",
    " - z < 0일 때 음숫값이 들어오므로 활성화 함수의 평균 출력이 0에 더 가까워진다. 그렇기 때문에 편향 이동이 감소하여 그래디언트 소실 문제를 줄여준다. 하이퍼파라미터 $\\alpha$는 z가 큰 음숫값일 때 ELU가 수렴할 값을 정의한다. 보통 1로 설정\n",
    " - z < 0이어도 그레이디언트가 0이 아니므로 죽은 뉴런을 만들지 않는다.\n",
    " - $\\alpha$ = 1 이면 이 함수는 z=0에서 급격히 변동하지 않으므로 z=0을 포함해 모든 구간에서 매끄러워 경사하강법의 속도를 높여준다.\n",
    " - 단점은 ReLU나 그 변종들보다 계산이 느리다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d91dfcee0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텐서플로우에서 ELU적용하기\n",
    "keras.layers.Dense(10, activation=\"elu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SELU(Scaled ELU) : 권터 클람바우어등의 2017년 논문에서 소개됨\n",
    "* 스케일이 조정된 ELU 활성화 함수의 변종\n",
    "* 완전 연결 층만 쌓아서 신경망을 만들고 모든 은닉층이 SELU 활성화 함수를 사용한다면 네트워크가 자기 정규화된다는것을 보였다.\n",
    "* 훈련하는 동안 각 층의 출력이 평균 0과 표준편차 1을 유지하는 경향이 있다. 이는 그레이디언트 소실과 폭주 문제를 막아준다.\n",
    "* 자기 정규화가 일어나기 위한 조건\n",
    " - 입력 특성이 반드시 표준화되어야 한다.\n",
    " - 모든 은닉층의 가중치는 르쿤 정규분포 초기화로 초기화되어야 한다. (케라스의 kernel_initializer=\"lecun_normal\")\n",
    " - 네트워크는 일렬로 쌓은 층으로 구성되어야 한다. 순환 신경망과 같은 순차적이지 않은 구조에 SELU를 사용하면 자기 정규화되는 것이 보장되지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TLDR : 어떠한 활성화 함수를 써야할까?\n",
    "\n",
    " - 일반적으로 ELU → LeakyReLU → ReLU → tanh → sigmoid 순으로 사용한다고 한다. cs231n 강의에서는 ReLU를 먼저 쓰고 , 그다음으로 LeakyReLU나 ELU 같은 ReLU Family를 쓰며, sigmoid는 사용하지 말라고 하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha와 scale은 평균 0과 표준 편차 1로 자기 정규화합니다\n",
    "# (논문에 있는 식 14 참조):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) *\\\n",
    "(2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*\\\n",
    " erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # 표준화된 입력\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun 초기화\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d8fb09400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100개의 은닉층과 SELU 활성화 함수를 사용한 패션 MNIST를 위한 신경망\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 1.3713 - accuracy: 0.4687 - val_loss: 0.9437 - val_accuracy: 0.6360\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.8343 - accuracy: 0.6928 - val_loss: 0.9943 - val_accuracy: 0.6464\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.6983 - accuracy: 0.7458 - val_loss: 0.6822 - val_accuracy: 0.7624\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.6252 - accuracy: 0.7700 - val_loss: 0.6009 - val_accuracy: 0.7784\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.5783 - accuracy: 0.7847 - val_loss: 0.5523 - val_accuracy: 0.7996\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그렇다면 대신 ReLU사용\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 884/1719 [==============>...............] - ETA: 5s - loss: 2.1080 - accuracy: 0.1922"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e5dce198a5d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m               metrics=[\"accuracy\"])\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m history = model.fit(X_train_scaled, y_train, epochs=5,\n\u001b[0m\u001b[0;32m      6\u001b[0m                     validation_data=(X_valid_scaled, y_valid))\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#그레이디언트 폭주나 소실 문제 발생\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))\n",
    "#그레이디언트 폭주나 소실 문제 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.3 배치 정규화\n",
    "\n",
    "* ELU와 함께 He 초기화를 사용하면 훈련 초기 단계에서 그레이디언트 소실이나 폭주 문제를 크게 감소시킬 수 있지만, 훈련하는 동안 다시 발생할 가능성이 있다.\n",
    "* 2015년 Sergety Ioffe와 Christian Szegedy는 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift'라는 논문에서 배치 정규화(BN, Batch Normalization)를 제안했다.\n",
    "* 배치 정규화는 각 층의 활성화 함수의 출력값 분포가 골고루 분포되도록 강제하는 방법으로, 각 층에서의 활성화 함수 출력값이 정규분포(Normal distribution)를 이루도록 하는 방법\n",
    " 1. 각 충에서 활성화 함수를 통과하기 전이나 후에 모델에 연산을 하나 추가한다.\n",
    " 2. 이 연산은 단순하게 입력을 원점에 맞추고 정규화한 다음, 각 층에서 두개의 새로운 파라미터로 결과값의 스케일을 조정하고 이동시킨다.\n",
    " 3. 하나는 스케일 조정에, 다른 하나는 이동에 사용\n",
    "* 학습하는 동안 이전 레이어에서의 가중치 매개변수가 변함에 따라 활성화 함수 출력값의 분포가 변화하는 내부 공변량 변화(Internal Covariate Shift) 문제를 줄이는 방법이 바로 배치 정규화 기법\n",
    "* 배치 정규화는 미니배치(mini-batch)의 데이터에서 각 특성(feature)별 평균(mean)과 분산(variance)을 구한 뒤 정규화(normalize)해준다.\n",
    "<img src = \"images/11_images/bn.png\" >\n",
    "\n",
    "* 입력 데이터를 원점에 맞추고 정규화하려면 알고리즘은 평균과 표준편차를 추정해야 한다. 이를 위해 현재 미니배치에서 입력의 평균과 표준편차를 평가해야한다.\n",
    "* 전체 알고리즘\n",
    "\n",
    " - Input : 미니배치 $B = \\{ x_1, x_2, \\dots, x_m \\}$ 개의 입력 데이터, 학습 될 파라미터인 $\\gamma, \\beta$\n",
    " - Output : $\\{ y_i = \\text{BN}_{\\gamma, \\beta}(x_i) \\}$\n",
    " \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*} \\mu _{ B } \\leftarrow \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ x_{ i } }  \\text{// mini-batch mean}  \\\\ \\sigma _{ B }^{ 2 }  \\leftarrow   \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ \\left( x_{ i }-\\mu _{ B } \\right) ^{ 2 } }  \\text{// mini-batch variance}  \\\\ \\hat { x } _{ i }  \\leftarrow   \\frac { x_{ i }-\\mu _{ B } }{ \\sqrt { \\sigma _{ B }^{ 2 }+\\varepsilon }}  \\text{// normalize} \\\\ y_{i}  \\leftarrow  \\gamma \\hat{x}_{i} + \\beta \\equiv \\text{BN}_{\\gamma, \\beta}(x_i) \\text{// scale and shift} \\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "   - $\\mu_B$ : 미니배치 $B$에 대한 평균\n",
    "   - $\\sigma_B$ : 미니배치 $B$에 대한 표준편차\n",
    "   - $m$ : 미니배치 데이터 개수\n",
    "   - $\\hat{x}_i$ : 평균이 0, 분산이 1로 정규화된 입력 데이터\n",
    "   - $\\gamma$ : 정규화된 데이터에 대한 스케일(scale) 조정 파라미터\n",
    "   - $\\beta$ : 정규화된 데이터에 대한 이동(shift) 조정 파라미터\n",
    "   - $\\varepsilon$ : 분모가 0이 되는 것을 막기 위한 작은 숫자 ($10^{-5}$)\n",
    "   - $y_i$ : $\\text{BN}$ 연산의 출력 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련하는 동안 배치 정규화는 입력을 정규화한 다음 스케일을 조정하고 이동시킨다.\n",
    "* 테스트 시에는 샘플의 배치가 아니라 샘플 하나에 대한 예측을 만들어야 한다. 이 경우 입력의 평균과 표준편차를 계산할 방법이 없다. 샘플의 배치를 사용한다 하더라도 매우 작거나 독립 동일 분포(Independent Identically Distributed, IID) 조건을 만족하지 못할 수도 있다.\n",
    "* 그래서 한가지 방법은 훈련이 끝난 후 전체 훈련 세트를 신경망에 통과시켜 배치 정규화층의 각 입력에 대한 평균과 표준편차를 계산하는 것이다.\n",
    "* 그러나 대부분 배치 정규화 구현은 층의 입력 평균과 표준편차의 이동 평균을 사용해 훈련하는 동안 최종 통계를 추정한다.(케라스의 BatchNormalization)\n",
    "* 즉 배치 정규화 층마다 네 개의 파라미터 벡터가 학습된다.\n",
    " - $\\gamma$(출력 스케일 벡터)와 $\\beta$(출력 이동 벡터)는 일반적인 역전파를 통해 학습된다.\n",
    " - $\\lambda$(최종 입력 평균 벡터)와 $\\omega$(최종 입력 표준 편차 벡터)는 지수 이동 평균을 사용하여 추정\n",
    " - $\\lambda$와 $\\omega$는 훈련하는 동안 추정되지만 훈련이 끝난 후에 사용된다.\n",
    " \n",
    "* 배치정규화의 장점\n",
    " - 논문에서 실험했던 모든 심층 신경망의 성능이 크게 향상 시켰다.\n",
    " - 그레이디언트 소실 문제가 크게 감소하여 수렴성을 가진 활성화 함수(하이퍼볼릭 탄젠트, 로지스틱)를 사용할 수 있다.\n",
    " - 가중치 초기화에 네트워크가 훨씬 덜 민감해진다.\n",
    " - 큰 학습률을 사용하여 학습 과정의 속도를 크게 높일 수 있다.\n",
    " - 오버피팅을 억제한다. BN이 마치 Regularization 역할을 하기 때문에 드롭아웃(Dropout)과 같은 규제기법에 대한 필요성이 감소한다. 하지만, BN로 인한 규제는 효과가 크지 않기 때문에 드롭아웃을 함께 사용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 케라스로 배치 정규화 구현하기\n",
    " - 은닉층의 활성화 함수 전이나 후에 BatchNormalization층을 추가하면 된다.\n",
    " - 모델의 첫 번째 층으로 배치 정규화 층을 추가할 수도 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#첫 번째 배치 정규화 층의 파라미터 / 두개는 역전파 훈련, 나머지 두개는 훈련 X\n",
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이동 평균을 업데이트\n",
    "bn1.updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수전에 BN을 적용해도 잘 동작합니다.\n",
    "#또한 BatchNormalization 층 이전의 층은 편향을 위한 항이 필요 없습니다.\n",
    "#BatchNormalization 층이 이를 무효화하기 때문입니다. \n",
    "#따라서 필요 없는 파라미터이므로 use_bias=False를 지정하여 층을 만들 수 있습니다:\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BatchNormalization 클래스는 조정할 하이퍼파라미터가 적다. 보통 기본값이 잘 동작하지만 momentum 매개변수를 변경해야 할 수 있다. 지수 이동 평균을 업데이트할 때 이 하이퍼파라미터를 사용\n",
    "* 새로운 값 v(현재 배치에서 계산한 새로운 입력 평균 벡터나 표준편차 벡터)가 주어지면 다음 식을 사용해 평균 $\\hat{v}$를 업데이트\n",
    "$$ \\hat{v} \\leftarrow  \\hat{v} * momentum + v  *  (1 - momentum) $$\n",
    " - 적절한 모멘텀 값은 일반적으로 1에 가깝다.\n",
    " \n",
    "* 중요한 다른 하이퍼파라미터는 axis\n",
    "* 정규화할 축을 결정, 기본값은 -1\n",
    "* 입력 배치가 2D면 각 입력 특성이 배치에 있는 모든 샘플에 대해 계산한 평균과 표준편차를 기반으로 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.4 그레이디언트 클리핑\n",
    "\n",
    "* 그레이디언트 클리핑(Gradient Clipping) : 역전파될 때 일정 임곗값을 넘어서지 못하게 그레이디언트를 잘라내어 폭주 문제를 완화하는 방법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#캐라스에서 옵티마이저를 만들 때 clipvalue와 clipnorm 매개변수를 지정\n",
    "optimizer = keras.optimizers.SGD(clipvalue = 1.0)\n",
    "model.compile(loss = \"mse\", optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이 옵티마이저는 그레이디언트 벡터의 모든 원소를 -1.0 과 1.0 사이로 클리핑\n",
    " - 이 기능은 그레이디언트 벡터의 방향을 바꿀 수 있다.\n",
    "* 그레이디언트 클리핑이 그레이디언트 벡터의 방향을 바꾸지 못하게 하려면 clipvalue 대신 clipnorm을 지정하여 노름으로 클리핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2 사전훈련된 층 재사용하기\n",
    "\n",
    "* 전이 학습(Transfer Learning) : 해결하려는 것과 비슷한 유형의 문제를 처리한 신경망이 이미 있는지 찾아본 다음 그 신경망의 하위층을 재사용하는 방법\n",
    "* 훈련 속도를 크게 높일 뿐만 아니라 필요한 훈련 데이터도 크게 줄여준다.\n",
    "<img src = \"images/11_images/pretrained.png\">\n",
    "\n",
    " - 원래 문제에서 사용한 것과 크기가 다른 이미지를 입력으로 사용한다면 원본 모델에 맞는 크기로 변경하는 전처리 단계를 추가\n",
    " - 일반적으로 전이학습은 저수준 특성이 비슷한 입력에서 잘 작동\n",
    " - 작업이 비슷할수록 (낮은 층부터) 더 많은 층을 재사용, 아주 비슷한 작업이라면 모든 은닉층을 유지하고 출력층만 교체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. 재사용하는 층을 모두 동결(경사 하강법으로 가중치가 바뀌지 않도록 훈련되지 않는 가중치로 만든다)\n",
    " 2. 모델을 훈련하고 성능을 평가\n",
    " 3. 맨 위에 있는 한두개의 은닉층의 동결을 해제하고 역전파를 통해 가중치를 조정하여 성능이 향상되는지 확인\n",
    " - 재사용 층의 동결을 해제할 때는 학습률을 줄이는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.1 케라스를 사용한 전이 학습\n",
    "\n",
    "* 패션 MNIST 훈련 세트를 두 개로 나누기\n",
    " - X_train_A: 샌달과 셔츠(클래스 5와 6)을 제외한 모든 이미지\n",
    " - X_train_B: 샌달과 셔츠 이미지 중 처음 200개만 가진 작은 훈련 세트\n",
    "\n",
    "* A세트(8개의 클래스를 가진 분류 문제)에서 모델을 훈련하고 이를 재사용하여 B세트(이진 분류)를 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_A.shape)\n",
    "print(X_train_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_A 학습\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "model_A.save(\"images/model/my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_B 학습\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_A를 로드하고, 이 모델의 층을 기반으로 model_B_on_A를 생성(출력층 제외 모든 층 재사용)\n",
    "model_A = keras.models.load_model(\"images/model/my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#층울 공유하는 과정에서 원본에 영향을 주지 않기 위해 clone 파일 만들기\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#처음 몇 번의 에포크 동안 재사용된 층을 동결하고 새로운 층에 적절한 가중치를 학습할 시간 제공\n",
    "#모든 층의 trainable 속성을 False로 지정하고 모델을 컴파일\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#최종 점수\n",
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.2 비지도 사전훈련\n",
    "\n",
    "* 레이블된 훈련 데이터가 많지 않은 복잡한 문제가 있는데, 아쉽게도 비슷한 작업에 대해 훈련된 모델을 찾을 수 없다고 가정했을 때 사용할 수 있는 방법은 비지도 사전훈련(Unsupervised Pretraining)이다.\n",
    "* 딥러닝 초기에는 층이 많은 모델을 훈련하는 것이 어려웠기 때문에 탐욕적 층 단위 사전훈련(Greedy layer-wise pretraining)이라는 기법을 사용\n",
    " - 하나의 층을 가진 비지도 학습 모델을 훈련(일반적으로 RBM)\n",
    " - 그 다음 이 층을 동결하고 그 위에 다른 층을 추가한 다음 모델을 다시 훈련\n",
    " - 이를 반복\n",
    "* 오늘날에는 일반적으로 한 번에 전체 비지도 학습 모델을 훈련하고(바로 세번째 단계를 시작), RBM 대신 오토인코더나 GAN사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.3 고속 옵티마이저 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련 속도를 높이는 네 가지 방법\n",
    " - 연결 가중치에 좋은 초기화 전략 적용하기\n",
    " - 좋은 활성화 함수 사용하기\n",
    " - 배치 정규화 사용하기\n",
    " - 사전훈련된 네트워크의 일부 재사용하기\n",
    " \n",
    "* 훈련 속도를 크게 높일 수 있는 또 다른 방법으로 표준적인 경사 하강법 옵티마이저 대신 더 빠른 옵티마이저를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  11.3.1 모멘텀 최적화\n",
    "\n",
    "* 모멘텀 최적화의 간단한 원리는 처음에는 느리게 출발하지만 종단속도에 도달할 때 까지 빠르게 가속되는 원리이다.\n",
    "* 이전 그레이디언트가 얼마였는지를 고려하여, 매 반복에서 현재 그레이디언트를 (학습률을 곱한 후) 모멘텀 벡터(Momentum Vector) m에 더하고 이 값을 빼는 방식으로 가중치를 갱신한다.\n",
    "* 즉, 그레이디언트를 속도가 아니라 가속도로 사용한다.\n",
    "* 일종의 마찰저항을 표현하고 모멘텀이 너무 커지는 것을 막기 위해 이 알고리즘에 모멘텀(Momentum)이라는 새로운 하이퍼 파라미터 $\\beta$가 등장\n",
    " - 모멘텀은 0~1 사이로 설정되며, 일반적인 값은 0.9이다.\n",
    "* 모멘텀 알고리즘\n",
    " - 1. $m \\leftarrow \\beta m - \\eta \\nabla_{\\theta}J(\\theta)$ \n",
    " - 2. $\\theta \\leftarrow \\theta + m$\n",
    "* 모멘텀 최적화는 경사 하강법보다 평편한 지역을 탈출하게 도와준다.\n",
    "* 모멘텀 최적화는 지역 최적점을 건너뛰도록 하는데도 도움이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스에서 모멘텀 최적화 구현\n",
    "optimizer = keras.optimizers.SGD(lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.2 네스테로프 가속 경사\n",
    "\n",
    "* 네스테로프 가속 경사(Nesterov Accelerated Gradient, NAG)는 현재 위치가 $\\theta$가 아니라 모멘텀의 방향으로 조금 앞선 $\\theta + \\beta m$에서 비용 함수의 그레이디언트를 계산하는 것\n",
    "* 네스테로프 가속 경사 알고리즘\n",
    " - 1. $m \\leftarrow \\beta m - \\eta \\nabla_{\\theta}J(\\theta + \\beta m)$ \n",
    " - 2. $\\theta \\leftarrow \\theta + m$\n",
    " \n",
    "* 원래 위치에서의 그레이디언트를 사용하는 것보다 그 방향으로 조금 더 나아가서 측정한 그레이디언트를 사용하는 것이 약간 더 정확할 것이다.\n",
    "* 시간이 조금 지나면 이 작은 개선이 쌓여서 NAG가 기본 모멘텀 최적화보다 확연히 빨라지게 된다.\n",
    "* NAG가 일반적으로 기본 모멘텀 최적화보다 훈련 속도가 빠르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#네스테로프 가속 경사\n",
    "optimizer = keras.optimizers.SGD(lr = 0.001, momentum=0.9, nesterov = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.3 AdaGrad\n",
    "\n",
    "* 가장 가파른 차원을 따라 그레이디언트 벡터의 스케일을 감소시켜 이 문제를 해결\n",
    "* AdaGrad 알고리즘\n",
    " - 1. $s \\leftarrow s + \\nabla_{\\theta}J(\\theta)$ ⓧ $\\nabla_{\\theta}J(\\theta)$\n",
    " - 2. $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}J(\\theta)$ / $\\sqrt{s + \\epsilon}$\n",
    " - ⓧ : 원소별 곱셈, / : 원소별 나눗셈\n",
    "   - 첫번쨰 단계는 파라미터 $\\theta$ 에 대한 비용 함수의 편미분을 제곱하여 누적\n",
    "   - 두번째 단계는 경사 하강법과 거의 같지만 차이점은 그레이디언트 벡터를 $\\sqrt{s + \\epsilon}$ 으로 나누어 스케일을 조정\n",
    "* 이 알고리즘은 학습률은 감소시키지만 경사가 완만한 차원보다 가파른 차원에 대해 더 빠르게 감소된다. 이를 적응적 학습률(Adaptive Learning Rate)이라고 부르며, 전역 최적점 방향으로 더 곧장 가도록 갱신되는 데 도움이 된다.\n",
    "* 학습률 하이퍼파라미터를 덜 튜닝해도 되는 장점이 있다.\n",
    "* AdaGrad는 간단한 2차방정식 문제에 대해서는 잘 작동되지만, 신경망을 훈련할 때 너무 일찍 멈추는 경우가 종종 있다. 그래서 심층 신경망에는 사용하지 말아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaGrad\n",
    "optimizer = keras.optimizers.Adagrad(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.4 RMSProp \n",
    "\n",
    "* AdaGrad는 너무 빨리 느려져서 전역 최적점에 수렴하지 못하는 위험이 있어서, RMSProp 알고리즘은 가장 최근 반복에서 비롯된 그레이디언트만 누적함으로써 이 문제를 해결\n",
    "* RMSProp 알고리즘\n",
    " - 1. $s \\leftarrow \\beta s + (1 - \\beta) \\nabla_{\\theta}J(\\theta)$ ⓧ $\\nabla_{\\theta}J(\\theta)$\n",
    " - 2. $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}J(\\theta)$ / $\\sqrt{s + \\epsilon}$\n",
    " - ⓧ : 원소별 곱셈, / : 원소별 나눗셈\n",
    "* 첫 번째 단계에서 지수 감소를 사용, 보통 감쇠율 $\\beta$ 는 0.9로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSProp 옵티마이저\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.5 Adam과 Nadam 최적화\n",
    "\n",
    "* 적응적 모멘트 추정(Adaptive Moment Estimation, Adam) : 모멘텀 최적화와 RMSProp의 아이디어를 합친 것으로, 모멘텀 최적화처럼 지난 그레이디언트의 지수 감소 평균을 따르고 RMSProp처럼 지난 그레이디언트 제곱의 지수 감소된 평균을 따른다.\n",
    "* Adam 알고리즘\n",
    " - 1.  $m \\leftarrow \\beta _{1}m -(1 - \\beta_{1}) \\nabla_{\\theta}J(\\theta)$\n",
    " - 2.  $s \\leftarrow \\beta _{2}s + (1 - \\beta_{2}) \\nabla_{\\theta}J(\\theta)$ ⓧ $\\nabla_{\\theta}J(\\theta)$\n",
    " - 3.  $\\hat{m} \\leftarrow \\frac{m}{1-\\beta_{1}^t}$\n",
    " - 4.  $\\hat{s} \\leftarrow \\frac{s}{1-\\beta_{2}^t}$\n",
    " - 5.  $\\theta \\leftarrow \\theta + \\eta \\hat{m}$ / $\\sqrt{\\hat{s} + \\epsilon}$\n",
    " - ⓧ : 원소별 곱셈, / : 원소별 나눗셈\n",
    "* 모멘텀 감쇠 하이퍼파라미터 $\\beta_{1}$은 보통 0.9로 초기화하고 스케일 감쇠 하이퍼파라미터 $\\beta_{2}$는 0.999로 초기화하는 경우가 많다.\n",
    "* Adam이 적응적 학습률 알고리즘이기 떄문에 학습률 하이퍼파라미터를 튜닝할 필요가 적다.\n",
    "* 기본값 $\\eta$ = 0.001을 일반적으로 사용하므로 경사 하강법보다 Adam이 사용하기 더 쉽다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam 옵티마이저\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam의 두가지 변종\n",
    "\n",
    "* AdaMax\n",
    " - Adam은 시간에 다라 감쇠된 그레이디언트의 노름_1(제곱합의 제곱근)으로 파라미터 업데이트의 스케일을 낮춘다.\n",
    " - AdaMax는 노름$_1$(제곱근의 제곱근)을 노름$_∞$으로 바꾼다.\n",
    " - 구체적으로 Adam 알고리즘의 2단계를 $s \\leftarrow max(\\beta_{2}s, \\nabla_{\\theta}J(\\theta))$ 로 바꾸고 4단계를 삭제한다.\n",
    " - 5단계에서 s에 비례하여 그레이디언트 업데이트의 스케일을 낮춘다. 시간에 다라 감쇠된 그레이디언트의 최댓값이다.\n",
    " - 실전에서 AdaMax가 Adam보다 더 안정적이다. Adma이 성능이 일반적으로 더 낫긴하지만, Adma이 잘 동작하지 않는다면 시도할 수 있는 옵티마이저 중 하나.\n",
    "\n",
    "* Nadam\n",
    " - Adam 옵티마이저에 네스테로프 기법을 더한 것으로 따라서 종종 Adam보다 조금 더 빠르게 수렴한다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaMax 옵티마이저\n",
    "optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "#Nadam 옵티마이저\n",
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.6 학습률 스케줄링\n",
    "\n",
    "* 학습 스케줄(Learning Schedule) : 훈련하는 동안 학습률을 감소시키는 전략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 거듭제곱 기반 스케줄링(Power Scheduling)\n",
    " - 학습률을 반복 횟수 t에 대한 함수 $\\eta (t) = \\eta_{0} / (1 + t/s)^c$로 지정\n",
    " - 거듭제곱 수 c(일반적으로 1로 지정), 스텝 횟수 s는 하이퍼 파라미터\n",
    " - 학습률은 각 스텝마다 감소된다. s번 스텝을 n번 진행한 뒤에 학습률은 $\\eta_{0} / n$로 줄어든다.\n",
    " - 처음에는 빠르게 감소하다가 점점 더 느리게 감소된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#거듭제곱 기반 스케줄링\n",
    "#lr = lr0 / (1 + steps / s)**c\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "decay = 1e-4\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 지수 기반 스케줄링(Exponential Scheduling)\n",
    " - 학습률을 $\\eta (t) = \\eta_{0} 0.1^{t/s}$ 로 설\n",
    " - 학습률이 s 스텝마다 10배씩 점차 줄어든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#지수 기반 스케줄링\n",
    "#lr = lr0 * 0.1**(epoch / s)\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이 스케줄 함수는 두 번째 매개변수로 현재 학습률을 받을 수 있다.\n",
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # 노트: 에포크마다 `batch` 매개변수가 재설정됩니다\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, lr * 0.1**(1 / s))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "lr0 = 0.01\n",
    "optimizer = keras.optimizers.Nadam(lr=lr0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "\n",
    "s = 20 * len(X_train) // 32 # 20 에포크 동안 스텝 횟수 (배치 크기 = 32)\n",
    "exp_decay = ExponentialDecay(s)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[exp_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = n_epochs * len(X_train) // 32\n",
    "steps = np.arange(n_steps)\n",
    "lrs = lr0 * 0.1**(steps / s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, lrs, \"-\", linewidth=2)\n",
    "plt.axis([0, n_steps - 1, 0, lr0 * 1.1])\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling (per batch)\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 구간별 고정 스케줄링(Piecewise Constant Scheduling)\n",
    " - 일정 횟수의 에포크 동안 일정한 학습률을 사용하고 그다음 또 다른 횟수의 에포크 동안 작은 학습률을 사용하는 식\n",
    " - 이 방법이 잘 동작할 수 있지만 적절한 학습률과 에포크 횟수의 조합을 찾으려면 이리저러 바꿔봐야한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#구간별 고정 스케줄링\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 성능 기반 스케줄링(Performance Scheduling)\n",
    " - 매 N 스텝마다 검증 오차를 측정하고 오차가 줄어들지 않으면 $\\lambda$배 만큼 학습률을 감소시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#성능 기반 스케줄링\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\", color='b')\n",
    "plt.tick_params('y', colors='b')\n",
    "plt.gca().set_xlim(0, n_epochs - 1)\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\n",
    "ax2.set_ylabel('Validation Loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 사이클 스케줄링(1 Cycle Scheduling)\n",
    " - 다른 방식과 대조적으로 1사이클은 훈련 절반 동안 초기 학습률 $\\eta_{0}$을 선형적으로 $\\eta_{1}$까지 증가시킨다. 그 다음 나머지 절반 동안 선형적으로 학습률을 $\\eta_{0}$까지 다시 줄인다.\n",
    " - 마지막 몇 번의 에포크는 학습률을 소수점 몇 째 자리까지 줄인다.\n",
    " - 최대 학습률 $\\eta_{1}$은 최적의 학습률을 찾을 때와 같은 방식을 사용해 선택하고 초기 학습률 $\\eta_{0}$은 대략 10배 정도 낮은 값을 선택한다\n",
    " - 모멘텀을 사용할 때는 처음에 높은 모멘텀으로 시작해서 훈련의 처음 절반 동안 낮은 모멘텀으로 줄어든다. 그 다음 다시 나머지 훈련 절반 동안 최대값으로 되돌린다. 마지막의 몇 번의 에포크는 최대값으로 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1사이클 스케줄링\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = len(X) // batch_size * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(len(X_train) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras 스케줄러\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],\n",
    "    values=[0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.4 규제를 사용해 과대적합 피하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.1 $ℓ_{1} 과 ℓ_{2}$ 규제\n",
    "\n",
    "* 신경망의 연결 가중치를 제한하기 위해 $ℓ_2$규제를 사용하거나 희소 모델을 만들기 위해 $ℓ_1$ 규제를 사용할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#케라스 층의 연결 가중치에 규제 강도 0.01을 사용하여 ℓ_2 규제 적용\n",
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#반복문을 피하기 위해 코드를 리팩터링\n",
    "#파이썬의 functions.partial() 함수를 사용하여 기본 매개변수 값을 함수 호출을 감싸기\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.2 드롭아웃\n",
    "\n",
    "* 매 훈련 스텝에서 각 뉴런은 임시적으로 드롭아웃될 확률 p를 가진다. 즉, 이번 훈련 스텝에는 완전히 무시되지만 다음 스텝에는 활성화 될 수 있다.\n",
    "* 하이퍼파라미터 p를 드롭아웃 비율(Dropout Rate)라고 하고 보통 10%~50% 사이를 지정한다.\n",
    "* 드롭아웃을 통해 입력값의 작은 변화에 덜 민감해진다. 결국 더 안정적인 네트워크가 되어 일반화 성능이 좋아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#드롭아웃 비율을 0.2를 사용한 드롭아웃 규제를 모든 Dense층 이전에 적용하는 코드\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 알파 드롭아웃\n",
    " - 드롭아웃 이후에도 자기-정규화 특성이 보장되도록 인풋의 평균과 분산을 원래값으로 유지하는 드롭아웃\n",
    " - 활성화를 무작위로 음수 포화값에 지정하여, 조정 지수 선형 유닛(Scaled Exponential Linear Units)에 대한 학습에 탁월하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.3 몬테 카를로 드롭아웃\n",
    "\n",
    "* 훈련된 드롭아웃 모델을 재훈련하거나 전혀 수정하지않고 성능을 크게 향상시킬 수 있는 몬테 카를로 드롭아웃(Monte Carlo Dropout, MCDropout)이라 불리는 기법 발견\n",
    "* 드롭아웃으로 만득 예측을 평균하면 일반적으로 드롭아웃이 없이 예측한 하나의 결과보다 더 안정적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MC드롭아웃\n",
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#드롭아웃을 끈 패션MNIST\n",
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#드롭아웃을 활성화\n",
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#드롭아웃 예측 확률\n",
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델이 훈련하는 동안 다르게 작동하는 층을 가지고 있을 때 MC 드롭아웃 코드\n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.4 맥스-노름 규제\n",
    "\n",
    "* 맥스-노름 규제(Max-Norm Regularization) : 각각의 뉴런에 대해 입력의 연결 가중치 w가 $||w||_2 ≤ r$이 되도록 제한한다.\n",
    "* r은 맥스-노름 하이퍼파라미터이고 $||ㆍ||_2 는 ℓ_2$는 노름을 나타낸다.\n",
    "* 전체 손실 함수에 규제 손실 항을 추가하지 않는다. 대신 일반적으로 매훈련 스텝이 끝나고 $||w||_2$를 계산하고 필요하면 w의 스케일을 조정한다.\n",
    "* r을 줄이면 규제의 양이 증가하여 과대적합을 감소시키는데 도움이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
